Run Large Language Models locally with Ollama on Nosana.

This template starts an Ollama service and pulls the specified model.

## Key Features
- Easy setup for running various LLMs.
- Simple API interaction via Ollama.
- GPU acceleration support.

## Configuration
- Port: 11434
- GPU: Required
- Model: gemma3:12b-it-qat 